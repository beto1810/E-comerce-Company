# -*- coding: utf-8 -*-
"""Data set 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RuhX0X-EnItXAbfdpRYtmxbw6VKg2u9S

# UPLOAD FILE
"""

#import lib
import pandas as pd
import numpy as np
import matplotlib as plt
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import dates
import datetime
print('Completed import lib')

from google.colab import drive
drive.mount('/content/drive')

#Upload dataset
customers = pd.read_csv('/content/drive/MyDrive/Final/De 1/dataset/customers_dataset.csv')
order_items = pd.read_csv('/content/drive/MyDrive/Final/De 1/dataset/order_items_dataset.csv')
order_payments = pd.read_csv('/content/drive/MyDrive/Final/De 1/dataset/order_payments_dataset.csv')
order_reviews = pd.read_csv('/content/drive/MyDrive/Final/De 1/dataset/order_reviews_dataset.csv')
orders = pd.read_csv('/content/drive/MyDrive/Final/De 1/dataset/orders_dataset.csv')
product_name_translation = pd.read_csv('/content/drive/MyDrive/Final/De 1/dataset/product_category_name_translation.csv')
products = pd.read_csv('/content/drive/MyDrive/Final/De 1/dataset/products_dataset.csv')

"""# Customers Dataset"""

customers.head()

customers.info()

#Checking the duplicated values of primary key column (customer_id), because number of customer_id is same with total data entries (99441), so we can conclude that there is not duplicated values

customers.nunique()

#Checking State unique 
customers['customer_state'].unique()

#Capitalize the first letter of city name
customers['customer_city'] = customers['customer_city'].str.title()

customers.head()

"""# Bảng Orders

"""

orders.head()

orders.info()

#Transforming the data type from object to datetime 
orders['order_purchase_timestamp'] = pd.to_datetime(orders['order_purchase_timestamp'], format = '%Y-%m-%d %H:%M:%S')
orders['order_approved_at'] = pd.to_datetime(orders['order_approved_at'], format = '%Y-%m-%d %H:%M:%S')
orders['order_delivered_carrier_date'] = pd.to_datetime(orders['order_delivered_carrier_date'], format = '%Y-%m-%d %H:%M:%S')
orders['order_delivered_customer_date'] = pd.to_datetime(orders['order_delivered_customer_date'], format = '%Y-%m-%d %H:%M:%S')
orders['order_estimated_delivery_date'] = pd.to_datetime(orders['order_estimated_delivery_date'], format = '%Y-%m-%d %H:%M:%S')

orders.info()

#Check Null Values
orders.isnull().sum()

#Check Percent of Null values. 
# Because the null values does not accounts much of total dataset ( about 3% is max), we can ignore or drop it
# However, The null values of these columns were also mean that the orders were not delivered to customer or carrier. So We can not drop them. 
orders.isnull().mean() * 100

orders['month_year'] = orders['order_purchase_timestamp'].map(lambda x: str(x.month) + "-" + str(x.year) )

orders.head()

"""# Bảng Order_Items"""

order_items.head()

order_items.describe()

order_items.info()

"""# Bảng **Order_payments**"""

order_payments.head()

order_payments.info()

order_payments['payment_type'].unique()

"""# Bảng **Order_reviews**"""

order_reviews.head()

order_reviews.info()

order_reviews['review_score'].value_counts()

order_reviews['review_creation_date'] = pd.to_datetime(order_reviews['review_creation_date'])
order_reviews['review_answer_timestamp'] = pd.to_datetime(order_reviews['review_answer_timestamp'])

order_reviews['review_creation_date'] = order_reviews.review_creation_date.dt.strftime('%m/%d/%Y')
order_reviews['review_answer_timestamp'] = order_reviews.review_answer_timestamp.dt.strftime('%m/%d/%Y')
order_reviews.head(5)

"""# Bảng **Products**

"""

products.head()

products.info()

products.describe(include = None)

# Min of product_weight_g = 0 , so we check this column to make sure there is nothing anomaly
products[products['product_weight_g']== 0]

#Check Null Values
products.isnull().sum()

#Check Null values of category name column
products[products['product_category_name'].isnull() == True]

#Check Null values of weight column
products[products['product_weight_g'].isnull() == True]

#Drop all 610 Null value rows , because they are not significant ( 610  rows >< 32951 total entries )
products = products.dropna()
products.isnull().sum()

products.isnull().sum()

#Check product_weight_g distribution
sns.distplot(products['product_weight_g'])

#Replace "0" values of weight to "median"
products['product_weight_g']= products['product_weight_g'].replace(0, products['product_weight_g'].median())

products.describe()

"""# Kiểm tra bảng **Products_Translation**"""

product_name_translation.head()

product_name_translation.info()

#Compare the product name of 2 table 
print(product_name_translation['product_category_name'].nunique())
print(products['product_category_name'].nunique())

"""Merge 2 table Products and Products Translation"""

product_summarize = products.merge(product_name_translation,how ='left', on = 'product_category_name' )

#Check Null values
product_summarize.isnull().sum()

product_summarize[product_summarize['product_category_name_english'].isnull() == True]

#Replace Null Value by Unspecified

product_summarize['product_category_name_english'] = product_summarize['product_category_name_english'].fillna(value ='Unspecified')

product_summarize.isnull().sum()

"""# Xuất File"""

#File customers
customers.to_csv('/content/drive/MyDrive/Final/De 1/customers_dataset.csv',index=False)

#File orders dataset
orders.to_csv('/content/drive/MyDrive/Final/De 1/orders_dataset.csv',index=False)

#File orders items
order_items.to_csv('/content/drive/MyDrive/Final/De 1/order_items_dataset.csv',index=False)

#File order payments
order_payments.to_csv('/content/drive/MyDrive/Final/De 1/order_payments_dataset.csv',index=False)

#File order review
order_reviews.to_csv('/content/drive/MyDrive/Final/De 1/order_reviews_dataset.csv',index=False)

#xuất cả 2 file product và produc_translation sau khi đã gộp lại
product_summarize.to_csv('/content/drive/MyDrive/Final/De 1/product_summarize_dataset.csv',index=False)